{
  "role": "ML Researcher (LLM Core)",
  "jd": "JOB DESCRIPTION:\nWe are training a new 7B parameter Foundation Model. We need a Researcher to lead the pre-training dynamics.\n\nResponsibilities:\n- Design the data curriculum and tokenization strategy.\n- Optimize training stability (Loss spikes, gradient explosions).\n- Implement distributed training using FSDP or Megatron-LM.\n- Publish research papers at NeurIPS/ICLR.\n\nRequirements:\n- PhD in CS/AI.\n- Experience pre-training large models (BERT, GPT, T5) from scratch.\n- Deep understanding of CUDA kernels and GPU memory communication.",
  "resume": "RESUME:\nDr. Lucas Vance\nPhD in Theoretical Deep Learning\n\nExperience:\n- PostDoc at University AI Lab (2020-Present)\n  - Published papers on \"Theoretical bounds of transformer convergence\".\n  - Proved a theorem on the limitations of Attention mechanisms.\n\nSkills: PyTorch, LaTeX, Mathematica, MATLAB.",
  "transcript": "INTERVIEW TRANSCRIPT (Duration: 50 mins)\n\nInterviewer: Dr. Vance, your paper on convergence bounds is fascinating. But here, we are training real models on 1,000 GPUs. Have you ever trained a model larger than 1B parameters?\n\nCandidate: Not personally. My work focuses on the mathematical properties of the architecture. The scale doesn't change the math. If the theorem holds for a small model, it holds for a large one. The engineering team handles the GPUs.\n\nInterviewer: In practice, scale changes everything. For example, we often see \"Loss Spikes\" mid-training. The loss suddenly jumps up and stays there. Based on your theoretical work, what causes this?\n\nCandidate: It's likely the learning rate is too high relative to the curvature of the loss landscape (the Hessian). You should just reduce the learning rate.\n\nInterviewer: We tried that. It didn't work. What else?\n\nCandidate: Maybe your batch size is too small, causing high variance in the gradient estimation.\n\nInterviewer: (Pushing harder) Actually, it often turns out to be \"dirty data\" in that specific batch, or bf16 numerical instability causing an overflow. How would you debug a `NaN` gradient in a distributed training run across 100 nodes?\n\nCandidate: I... I haven't done distributed debugging. I usually run my experiments on a single workstation. Can't you just print the gradients?\n\nInterviewer: Printing gradients on 100 nodes creates terabytes of logs instantly. We need a more systematic approach. Let's talk about Tokenization. We are training on code and text. How should we handle the vocabulary?\n\nCandidate: You should use BPE (Byte Pair Encoding). It's standard.\n\nInterviewer: Sure, but for Code, whitespace matters. For Math, digits matter. Standard BPE merges digits (e.g., \"2023\" becomes one token). This makes the model bad at arithmetic. How would you modify the tokenizer?\n\nCandidate: I haven't looked into the specific tokenization of digits. I assume the model learns it eventually. My research assumes the input is already a vector space.\n\nInterviewer: Okay. Last question. We use FSDP (Fully Sharded Data Parallel). Explain how FSDP saves memory compared to DDP (Distributed Data Parallel).\n\nCandidate: Distributed computing is really an implementation detail. I believe it splits the data?\n\nInterviewer: DDP splits the data. FSDP splits the *model weights and optimizer states*. It's critical for training anything larger than GPU memory.\n\nCandidate: I see. Well, as I said, I design the architectures. I expect there are engineers to handle the sharding."
}
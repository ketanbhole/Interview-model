{
  "role": "AI Engineer (LLM Ops)",
  "jd": "JOB DESCRIPTION:\nWe are building an Enterprise Knowledge Assistant. We need an AI Engineer to scale our RAG pipeline.\n\nResponsibilities:\n- Build production-grade RAG pipelines using LangChain/LlamaIndex.\n- Optimize vector search (Pinecone, Milvus, Weaviate).\n- Fine-tune open-source LLMs (Llama-3, Mistral) for specific domains.\n- Handle deployment and latency optimization (vLLM, TGI).\n\nRequirements:\n- 3+ years in NLP/ML Engineering.\n- Deep understanding of Embedding models and Semantic Search.\n- Experience with Quantization (GPTQ, AWQ) for efficient inference.\n- Strong Python & Docker skills.",
  "resume": "RESUME:\nSanjay Gupta\nAI Engineer | 4 Years Experience\n\nExperience:\n- AI Lead at FinTech Startup (2021-Present)\n  - Built a chatbot handling 10k queries/day using RAG.\n  - Reduced hallucination rate by 40% using Hybrid Search (BM25 + Dense).\n  - Deployed Llama-2-13B on A10G GPUs using vLLM, achieving <500ms TTFT.\n\n- Machine Learning Engineer at RetailCorp (2019-2021)\n  - Deployed recommendation systems using XGBoost and TensorFlow.\n\nSkills: Python, PyTorch, HuggingFace, LangChain, Docker, Kubernetes, AWS.",
  "transcript": "INTERVIEW TRANSCRIPT (Duration: 40 mins)\n\nInterviewer: Hi Sanjay. Let's talk about the chatbot you built. A lot of people build RAG demos, but production is different. What was the biggest challenge you faced moving from prototype to production?\n\nCandidate: The biggest hurdle was \"Retrieval Accuracy\". In the demo, we just chunked text by 500 characters and threw it into Pinecone. It worked fine for simple questions. But in production, users asked complex questions like \"Compare the Q3 revenue of 2021 vs 2022\". The simple chunking split the tables in half, so the LLM got garbage context.\n\nInterviewer: That's a classic problem. How did you solve it?\n\nCandidate: We moved to a \"Parent-Child\" indexing strategy. We chunked the documents into small \"Child\" chunks (200 tokens) for the vector search to find the precise match. But when we retrieved the context for the LLM, we fetched the \"Parent\" chunk (1000 tokens) that surrounded it. This gave the LLM the full context window without diluting the search vectors. We also implemented \"Recursive Retrieval\" for the financial tables specifically, converting them to Markdown format before embedding.\n\nInterviewer: Excellent. You mentioned Hybrid Search in your resume. Why did you need BM25? Isn't semantic search enough?\n\nCandidate: Semantic search is great for concepts, but terrible for specific keywords. In FinTech, if a user searches for \"Error Code 503\", a semantic model might return generic documents about \"Server Failures\". We needed it to find the exact document titled \"Error 503\". So we used Hybrid Search: we run a sparse keyword search (BM25) and a dense vector search simultaneously, then use a Reciprocal Rank Fusion (RRF) algorithm to re-rank the results. This boosted our retrieval recall by about 15%.\n\nInterviewer: Let's talk inference. You deployed Llama-2-13B. Why vLLM? Why not just standard HuggingFace pipeline?\n\nCandidate: Throughput. Standard HF pipeline processes requests sequentially. vLLM uses \"PagedAttention\", which manages the KV cache memory much more efficientlyâ€”kind of like virtual memory in an OS. It allows continuous batching. We could handle 10 concurrent users on a single A10G GPU without the latency spiking.\n\nInterviewer: Can you explain how PagedAttention works to a junior engineer?\n\nCandidate: Sure. Imagine the KV cache is a long text. Normally, you have to reserve a contiguous block of memory for the whole response, even if you don't know how long it will be. This wastes memory (fragmentation). PagedAttention breaks the KV cache into non-contiguous blocks (pages). The GPU can just grab any free block in memory when it needs it. This means less wasted RAM, which means we can fit bigger batches.\n\nInterviewer: Perfect explanation. Last question: How do you handle PII (Personally Identifiable Information) in the RAG pipeline?\n\nCandidate: We have a pre-processing scrubber. Before any text goes into the Vector DB, we run it through a Microsoft Presidio entity extractor. It detects names, SSNs, and emails and replaces them with placeholders like <PERSON_1>. We persist the mapping in a separate secure Redis store if we ever need to re-hydrate it, but the LLM never sees the raw PII."
}